{
    "version": "https://jsonfeed.org/version/1",
    "title": "気合でなんとか",
    "home_page_url": "https://ningensei848.github.io/",
    "description": "Kiai (@ningensei848) が日々を生きた証",
    "items": [
        {
            "id": "/2022/03/08/",
            "content_html": "<p>改めて，Netkeiba からスクレイピングをやっていく．\nPython でやるのは，リクエストに間隔を開ける都合上，多少時間がかかっても問題がないことや，DataFrame 系の資産を使い回せることが利点として挙げられる</p><p><a href=\"https://github.com/Ningensei848/ml4keiba\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/Ningensei848/ml4keiba</a></p><p><a href=\"https://github.com/Ningensei848/ml4keiba\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://4.bp.blogspot.com/-7KSDS7fjQZU/U1T4Hfdp7aI/AAAAAAAAfds/kxPMlCXrIkk/s200/seiza13_hebitsukai.png\" alt=\"ギリシャ神話に出てくる蛇を持つ医者アスクレーピオス（へびつかい座）\"></a></p><p>前回までの反省として，<strong>何も考えずひたすらにスクレイピングしていた</strong> というものが挙げられる．すなわち，不要なデータまでも「必要かもしれない」と集めて時間を浪費していた．これは，リストを作ってそれを一つずつ実行する設計になっていたことが原因だ．</p><p>例えば本日はちょうど「弥生賞ディープインパクト記念」をやっている．\nこのレースの予想をデータ分析によって行う場合に，「天皇賞（春）」のデータが必要だろうか？</p><p>最初から「完璧」を目指して作ろうとするからポンコツ不完全にしかならないという現実があるので，実際のレース時期に合わせてちいさくはじめていくべきだろう．</p><p>今回で言えば，弥生賞に出てくる馬それぞれの過去のレースを探ったり，過去の弥生賞についてデータを浚ったりするのが常道といえる．</p><h2 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"てがかり\">てがかり<a class=\"hash-link\" href=\"#てがかり\" title=\"Direct link to heading\">​</a></h2><p>まず，レースに出走する馬の一覧を入手する必要がある．</p><p>netkeiba では，<code>YYYYPPNNDDRR</code> という ID でレースごとの情報が管理されている．</p><ul><li><code>YYYY</code>: 開催年度</li><li><code>PP</code>: 会場コード</li><li><code>NN</code>: N 回目</li><li><code>DD</code>: 第 D 日</li><li><code>RR</code>: 第 R レース</li></ul><p>といった具合である．</p><p>過去の調査では，<code>PP</code> が厄介なことに，「地方」「海外」も雑多に含まれることがあり，単にインクリメントしているわけではないらしい．</p><p>（と，これを調べる過程で netkeiba.com が設立されたのが 1999 年末だということを知った，<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>そろそろ四半世紀にもなる上に前世紀からデータ提供をやってるんだからすげぇ）</p><p><a href=\"https://ascii.jp/elem/000/000/306/306735/\" target=\"_blank\" rel=\"noopener noreferrer\">ASCII.jp：ネットドリーマーズ、競馬のポータルサイト“netkeiba.com”を開設</a></p><p>これを書いているのは火曜日だが，週末に行われるレースについては現時点で出走馬も枠順も決まっていなかった．ただし週末に行われる重賞レースに登録されている馬の一覧は見ることができた．調べてみると netkeiba においては，以下のようなスケジュールで情報が書き換わるようだ：</p><ul><li>前週の日曜（G1 は前々週）　特別レース登録馬を公開</li><li>（netkeiba 独自）水曜 20 時ごろ　取材などからわかった水曜時点で出走意思のある馬（想定馬一覧）</li><li>木曜 16 時ごろ　　　　出走馬確定</li><li>レース前日 10 時ごろ　枠順確定</li></ul><p><a href=\"https://www.jra.go.jp/kouza/yougo/w333.html\" target=\"_blank\" rel=\"noopener noreferrer\">特別レース（特別競走）</a> とは，\"一般競走と違って、特別登録を必要とする競走。特別競走には、現在の中央競馬ではすべてレース名がつけられている。また重賞競走も特別競走のなかに含まれる\" ものであるらしい．また<a href=\"https://www.jra.go.jp/kouza/yougo/w320.html\" target=\"_blank\" rel=\"noopener noreferrer\">一般競走</a>とは，特別競走以外，すなわち新馬戦やオープン戦，条件戦などのことを指す<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>．</p><p>つまり，おおよそほとんどの場合において，木曜日の夜あたりから情報収集を始めるのが良いということがわかる．\nまた，枠順が決まらないことには予想も固まらないことを考えると，レース前日の夕方から準備し始めても十分に間に合う．\n金曜＋土日でレース本番への対策を行ない，それ以外の日には別の情報収集＋メンテナンスや振り返りというペース配分になるだろう．</p><p>（※これは中央競馬だけにフォーカスした場合の話で，平日もガンガン走っている地方競馬はまた別の話）</p><h2 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"出走馬一覧をシードとして\">出走馬一覧をシードとして…<a class=\"hash-link\" href=\"#出走馬一覧をシードとして\" title=\"Direct link to heading\">​</a></h2><p>netkeiba では，出走馬ごとに ID が振られており，それによって血統や戦績，厩舎や騎手などの情報を管理している．</p><p>日本生まれ，かつ出生情報がきちんと揃っている場合には <code>https://db.netkeiba.com/horse/YYYYXXXXXX</code> と表記される（<code>YYYY</code> は生年）．\n一方で海外産馬などの都合で情報が不明瞭な場合は <code>https://db.netkeiba.com/horse/000a00033a</code> といった ID が振られている（ちなみにこの URL は<a href=\"https://db.netkeiba.com/horse/000a00033a\" target=\"_blank\" rel=\"noopener noreferrer\">サンデーサイレンス</a>）．\n<code>000a</code> はほぼ共通だが，それ以外についてはひと目見ただけではあまり共通項が見えてこない．</p><hr><p>各馬について，スクレイピングの対象となるのは以下の５つのページである：</p><ul><li><code>/</code>: プロフィール</li><li><code>/result</code>: 競走成績</li><li><code>/ped</code>: 血統</li><li><code>/sire</code> or <code>/mare</code>: 産駒の競走成績（繁殖入りした馬のみ）</li></ul><h3 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"プロフィール\">プロフィール<a class=\"hash-link\" href=\"#プロフィール\" title=\"Direct link to heading\">​</a></h3><p>各馬のトップページにアクセスするとまっ先に目に入るのがプロフィールである．\n生まれに関する基礎情報や写真なんかもおいてあるし，他のページに詳しく掲載される情報も概要がまとめてある．</p><p>ここで収集すべきは，<code>div.horse_title</code>, <code>div.db_photo_box&gt;img</code>, <code>dl.tekisei&gt;dd&gt;table.tekisei_table</code>, <code>div.db_prof_table</code> の４箇所だ．\nそれぞれ，「名前」「写真」「適正評価」「プロフィール」が掲載されている．</p><p>また，「繁殖入りできたかどうか？」の判定のために〈産駒成績〉タブの有無も見つけられるようにしたい．\n<code>ul.db_detail_menu</code> を探るといいだろう．</p><h3 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"競走成績\">競走成績<a class=\"hash-link\" href=\"#競走成績\" title=\"Direct link to heading\">​</a></h3><p>競走成績一覧のページでは，その馬が一つでもレースに出ていれば，結果が表として出力される．\n<code>table.db_h_race_results</code> を収集すればいいだろう．\n未出走の場合や海外産馬の場合には表がないこともある．例外処理には気をつけたい．</p><h3 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"血統\">血統<a class=\"hash-link\" href=\"#血統\" title=\"Direct link to heading\">​</a></h3><p>こちらも同じく真ん中にデカデカと血統表が出力される．\n<code>table.blood_table</code> を収集すればいいだろう．\n名前等はこの時点では収集せず……と思ったが，後々必要になる気もしてきた．</p><p>また，兄弟等の近親についても情報があるが，敢えてこれを探りに行くのは骨が折れる．\nある程度データを集めてから，自前で謹慎を探せるようにするほうが良いと思われる．</p><p>表の組み方が特殊なことに留意．</p><h3 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"産駒成績\">産駒成績<a class=\"hash-link\" href=\"#産駒成績\" title=\"Direct link to heading\">​</a></h3><p>牡馬であれば <code>/sire</code>, 牝馬であれば <code>/mare</code> のページを持つ場合がある．\nこれは，繁殖入りできるほど血統的に期待される，或いは競走成績が良かった馬ということで，勝ち馬予想のためには必要不可欠な要素ではある．\nが，あくまで統計的な情報でしかなく，最初から予想アルゴリズムに組み込むのは難しいかもしれない．</p><p>産駒成績のページの存在判定は各馬のトップページでもタブを見ればできるはずなので，一旦保留して関知しないこととする．</p><h2 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"一旦まとめ\">一旦まとめ<a class=\"hash-link\" href=\"#一旦まとめ\" title=\"Direct link to heading\">​</a></h2><p>netkeiba からスクレイピングしてくるための検討をした．</p><p>スクレイピング処理の部分と LOD 化する部分の話がまだ書けていないが，一旦 Puslish しておく．\n（まぁどうなるかわからないが，前編ということとする）</p><p>Python によるスクレイピング処理とはすなわち， (1) <code>requests</code> （あるいは <code>aiohttp</code> ）でページコンテンツを取得し，(2) それを BeautifulSoup <sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup> にして (3) <code>pandas</code> でテーブルとか引っこ抜くことである．\nこの辺は以前に何度もやっているのでそのへんの資産を使い回せたら嬉しいな……</p><p>LOD 化するというのは，どうしようかまだ悩むところである．\nオープンデータにすることを考えると，いちいち事前知識が必要になる TTL 形式のみで提供するのは避けたい．\nとなると JSON-LD か？となるがこれもこれでファイル容量の無駄遣いが大きい気がする……</p><p>TSV で提供 &amp; TTL への変換ツールも提供という形がもっともスマートな解だと信じたい．</p><div class=\"footnotes\"><hr><ol><li id=\"fn-1\">というかそんな時期の記事にまだアクセスできる ASCII の根性がスゲェ，尊敬に値する<a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li><li id=\"fn-2\"><a href=\"https://www.jra.go.jp/kouza/yougo/c10020.html\" target=\"_blank\" rel=\"noopener noreferrer\">レースの種類、条件など（競馬用語辞典）JRA</a> を参照のこと<a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li><li id=\"fn-3\">現時点 (2022/03/08) の<a href=\"https://pypi.org/project/beautifulsoup4/4.10.0/\" target=\"_blank\" rel=\"noopener noreferrer\">最新版は Python 3.8 前提の 4.10.0</a> である（なお日本語訳は 4.2.0 までしかない模様）<a href=\"#fnref-3\" class=\"footnote-backref\">↩</a></li></ol></div>",
            "url": "https://ningensei848.github.io/2022/03/08/",
            "title": "netkeiba のデータをスクレイピングして LOD 化する（前編）",
            "summary": "改めて，Netkeiba からスクレイピングをやっていく．",
            "date_modified": "2022-03-08T00:00:00.000Z",
            "author": {
                "name": "Kiai",
                "url": "https://twitter.com/Ningensei848"
            },
            "tags": [
                "python",
                "スクレイピング",
                "競馬"
            ]
        },
        {
            "id": "/2022/03/03/",
            "content_html": "<p>もともとは Next.js の知見を貯めるために vercel では普通のアプリとして，GitHub Pages では SSG としてブログをつくる予定だった．が， Docusaurus で全然いいしむしろデザイン対応の手間を考えたら Next.js で全部自分でやるのは（無限にこだわり続けてしまって）完成しない！という結論を得た．</p><h2 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"そもそも\">そもそも<a class=\"hash-link\" href=\"#そもそも\" title=\"Direct link to heading\">​</a></h2><p>Docusaurus もこれまで $\\alpha$ 版には触れてきたが，Next.js における <code>getStaticProps</code> がないからローカルのファイルデータのやり取りができなくて辛い，と思い込んでしまっていた．しかし，実際にはプラグインを時前実装することで実現できることがわかった（それも正直どうなのって感じだが）</p><p>Docusaurus と Next.js のハイブリッドも検討したが，あまり親和性がないと言うか，そこまでしてやるメリットがないとは感じていた．</p><p>改めて，Next.js を使うのは ISR をやれるからという利点があるときだけで，そうでないときは Docusaurus を使ったほうが後々の拡張性が高いように感じた（特にレイアウトやらダークモードなどデザイン苦手マンには）</p><p>で，この GitHub Pages は，その性質上今後も SSG のみ対応すると思われる．じゃあ Next.js に縋り付く意味なくない？よって Docusaurus への移行を決意した．</p><h2 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"今後\">今後<a class=\"hash-link\" href=\"#今後\" title=\"Direct link to heading\">​</a></h2><p>なにかしらフィーチャーすべきものについては，「ドキュメント」をつくってまとめる．幸い「Multi Sidebar」なるものはすでにある（上部バーのドロップダウンがすぐに作れてアド）</p><p>ブログは，ルーティングをもう少し留意する必要はあると思いつつも，まぁこれでいいかという妥協点を保っている．そんなに何度も何度も更新するわけではないし……</p><h2 class=\"anchor anchorWithStickyNavbar_mojV\" id=\"最後に\">最後に<a class=\"hash-link\" href=\"#最後に\" title=\"Direct link to heading\">​</a></h2><p>はやいとこいくつか記事作って Google Adsense 通るようにしたいね</p><hr><p>↓ 　これは URL 貼り付けるだけでツイートが埋め込み表示されて便利ね～というデモンストレーションです</p><blockquote class=\"twitter-tweet\" align=\"center\" data-width=\"550\" data-lang=\"ja\" data-dnt=\"true\"><p lang=\"ja\" dir=\"ltr\">余裕をもってバスに乗る🚌</p>— ありがとう上木敬🌓 (@Ningensei848) <a href=\"https://twitter.com/Ningensei848/status/1477068019612659712?ref_src=twsrc%5Etfw\" target=\"_blank\" rel=\"noopener noreferrer\">2022年1月1日</a></blockquote><p>↓ 　あとこの辺に SNS 向けの共有ボタンがいくつか設置されてるはず</p>",
            "url": "https://ningensei848.github.io/2022/03/03/",
            "title": "個人ブログを Docusaurus で再始動する",
            "summary": "もともとは Next.js の知見を貯めるために vercel では普通のアプリとして，GitHub Pages では SSG としてブログをつくる予定だった．が， Docusaurus で全然いいしむしろデザイン対応の手間を考えたら Next.js で全部自分でやるのは（無限にこだわり続けてしまって）完成しない！という結論を得た．",
            "date_modified": "2022-03-04T00:00:00.000Z",
            "author": {
                "name": "Kiai",
                "url": "https://twitter.com/Ningensei848"
            },
            "tags": [
                "日記"
            ]
        }
    ]
}