<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://ningensei848.github.io/</id>
    <title>気合でなんとか</title>
    <updated>2022-03-23T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://ningensei848.github.io/"/>
    <subtitle>Kiai (@ningensei848) が日々を生きた証</subtitle>
    <icon>https://ningensei848.github.io/img/favicon.ico</icon>
    <rights>Copyright © ningensei848, 2022</rights>
    <entry>
        <title type="html"><![CDATA[netkeiba のデータをスクレイピングして LOD 化する（３）]]></title>
        <id>/2022/03/23/</id>
        <link href="https://ningensei848.github.io/2022/03/23/"/>
        <updated>2022-03-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[前回はスクレイピング効率を高めるためにプロキシサーバを作ろうという試みを行なって終わった．]]></summary>
        <content type="html"><![CDATA[<p>前回はスクレイピング効率を高めるためにプロキシサーバを作ろうという試みを行なって終わった．
今回は具体的にどのようにデータを集めるか検討する．</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="race_id"><code>race_id</code><a class="hash-link" href="#race_id" title="Direct link to heading">​</a></h2><p>前回でも少し触れたように，netkeiba では <code>/top/race_list_sub.html</code> に対して <code>?kaisai_date=YYYYMMDD</code> を投げて HTML の断片を受け取っている．
この URL に対してスクレイピングをかければ，うまいことレースの一覧が得られるということである．</p><p>この URL の嬉しい点として，起点が <code>YYYYMMDD</code> になっているところである．
例えば本日の日付 <code>20220315</code> を投げれば今日開催されるレースが返ってくる………それはつまり，毎日繰り返し定期的にデータを集められるようになるということだ．</p><p>また嬉しいことに，JRA だけでなく NAR（地方競馬）についても同様の方法でレース一覧情報が提供されていた．
こちらは JRA と異なり平日もバンバン開催されているので，試行回数を増やせるという点ではアドバンテージとなりうるだろう．</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="netkeiba-の文字コード">netkeiba の文字コード<a class="hash-link" href="#netkeiba-の文字コード" title="Direct link to heading">​</a></h3><p>「競馬の予測」に使えるのは「レース結果」ではなくレース直前の情報だが，それは <code>https://{race|nar}.netkeiba.com/race/shutuba.html?race_id=YYYYPPNNDDRR</code> で得ることができる．
ただし，気をつけるべきことがあった．
それは，netkeiba のページにおける文字コードが <strong>EUC-JP</strong> に指定されているということである．</p><p>たかが文字コード，されど文字コード…… 主に Python でのスクレイピングかつ対象が日本語ページであるとを考えると，これは死活問題である．
おそらくは <a href="https://requests-docs-ja.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><code>requests</code></a> + <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc" target="_blank" rel="noopener noreferrer"><code>BeautifulSoup</code></a> の組み合わせでスクリプトを書くだろうが，これらの基本文字コードは <code>utf-8</code> である．
すなわち，なんにも意識せずリクエストを投げるとすぐ文字化けしたソースを見ることになる．</p><p>もし id だけを必要とする場合には，この問題は気にならない．
必要な要素が全て英数字記号のみで構成されているため文字化けしないからだ．
ネックとなるのは日本語情報をデータとして取得したいときだろう．</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="当日にならないと得られない情報">当日にならないと得られない情報<a class="hash-link" href="#当日にならないと得られない情報" title="Direct link to heading">​</a></h3><p><code>shutuba.html</code> では，枠・馬番・馬体重（＋オッズ）も得られるが，これらは直前になってから出ないと判明しないデータである．
すなわち，（１）前日までにスクレイピングした情報（２）レースの３０分前にスクレイピングした情報では<strong>情報量が異なる</strong>．</p><p>最も重要と思われる馬体重の発表は，おおよそ発走時刻の一時間前であるようだ．
十分に余裕があるが，一方でこれは何をトリガーにスクレイピングすればよいだろうか……？
あくまで目安の時間であるため，きっかり一時間前にリクエストを投げても結果が帰らないかもしれない．
一旦これは保留とする．</p><p>（賞味期限が極めて短いデータであるため，GitHub へ保存するよりは，一旦 Firestore 等を経由しておくのが賢いかも……？）</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="前日までに得られる情報">前日までに得られる情報<a class="hash-link" href="#前日までに得られる情報" title="Direct link to heading">​</a></h3><p>出走登録している馬の一覧を得ることができる．
ここからさらに前日あたりで本登録となり，出走できない馬も出てくる．
まぁそれはそれで致し方ないので，出走候補について全て情報を集めておくのが良いだろう．</p><p>コードを書く方針としては，すでに開催されたレースについても <code>shutuba.html</code> は見られるのでそれをもとに処理を書き，適宜例外処理を追加していく感じにすれば良いと思う．</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="horse_id"><code>horse_id</code><a class="hash-link" href="#horse_id" title="Direct link to heading">​</a></h2><p>すべての馬に <code>/</code>, <code>/result</code>, <code>/ped</code> があり，繁殖入りできた場合には牡馬であれば <code>/sire</code>, 牝馬であれば <code>/mare</code> が存在する．
繁殖実績については現状の優先度は高くないので一旦置いておくとして，各馬のプロファイルは逐次集める必要がある．</p><p><code>/</code> と <code>/result</code> を見比べてみたところ，どちらにも同じ戦績テーブルが配置されていたため，<code>/result</code> へのスクレイピングは省略できそうだ．</p><p><code>/</code> （トップページ）では，各馬のプロファイルと戦績を取得できる．
後々ページコンテンツを充実させることを考えると，写真 URL も引っこ抜いてくるのがよさそうだ．</p><p><code>/ped</code> については，テーブルの形がだいぶイレギュラーなもので自前ではうまくパースできなかった．
代わりに，<code>pandas</code> の <code>read_html</code> を使ってテーブルを引っこ抜き，その構造を利用してうまいこと「世代ごと」にイテレーションできるようにした．
つまり，行で回すのではなく，列でループ処理できるようにした．
こうすれば，父母，祖父母……の処理が比較的簡単に実装できる．</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="データ構造">データ構造<a class="hash-link" href="#データ構造" title="Direct link to heading">​</a></h2><p>各馬のプロファイル，戦績，血統なんかも GET できたのだからそれをファイルデータとして保存しておきたい．
これまではうまく一行に押し込んたうえで，それを生年ごとに連ねて一つの TSV ファイルで管理していた．
しかし今後同時並列に複数箇所でデータ更新が走りうることを考えると，ACID 特性を備えていないのは手痛い．
（まぁ素直に DB 使えってツッコミはあるが，あくまで GitHub に残すことを中心に考える）</p><p>ファイル数が膨大になってしまうのを覚悟で，各馬ごとに一ファイルを割り当てる方針とする．
こうすれば，ファイル更新は最小限で済む．</p><p>次に問題になるのがフォーマットだが，サイズを考えると TSV が挙げられる．
しかし，たかが一行のデータのために TSV を使うのはもったいない（そうするならまとめろという話にもなる）．
ここはもっと柔軟な表現を持つ構造化データを用いるべきだと考えた．
サイズを考慮すると YAML が候補に上がるが，どの言語でも標準的に扱えるかと言うとそうではない．
じゃあやっぱり JSON か……とも思ったが，単に JSON を扱うだけでは芸がない，もとい LOD を謳うのに <a href="https://json-ld.org/" target="_blank" rel="noopener noreferrer"><strong>JSON-LD</strong></a> を無視するのはいただけない．</p><p><strong>各 ID ごとにファイルを作って管理，かつフォーマットとしては JSON-LD を用いるものとする</strong> というのが，今後の基本方針となった．</p><p>もちろん Virtuoso ではこのままでは使えないので，ローカルで <code>.ttl</code> へ変換するスクリプトも用意することになる．</p>]]></content>
        <author>
            <name>Kiai</name>
            <email>k.kubokawa@klis.tsukuba.ac.jp</email>
            <uri>https://twitter.com/Ningensei848</uri>
        </author>
        <category label="python" term="python"/>
        <category label="スクレイピング" term="スクレイピング"/>
        <category label="競馬" term="競馬"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[LOD Challenge 2021 授賞式に参加した]]></title>
        <id>/2022/03/13/</id>
        <link href="https://ningensei848.github.io/2022/03/13/"/>
        <updated>2022-03-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[LOD チャレンジ 2021に昨年秋～年末にかけて制作したものを応募した．]]></summary>
        <content type="html"><![CDATA[<p><a href="https://2021.lodc.jp/" target="_blank" rel="noopener noreferrer">LOD チャレンジ 2021</a>に昨年秋～年末にかけて制作したものを応募した．
本命の方は受賞を逃したが，<a href="https://zenn.dev/ningensei848/articles/virtuoso_on_gcp_faster_with_cos" target="_blank" rel="noopener noreferrer">副産物として生まれた記事</a>が<a href="https://2021.lodc.jp/awardPressRelease2021.html" target="_blank" rel="noopener noreferrer">LOD プロモーション賞</a>を受賞した．</p><p><a href="https://zenn.dev/ningensei848/articles/virtuoso_on_gcp_faster_with_cos" target="_blank" rel="noopener noreferrer">https://zenn.dev/ningensei848/articles/virtuoso_on_gcp_faster_with_cos</a></p><blockquote><p>RDF トリプルストア構築のハードルを下げることは、LOD の普及のための重要な要素の一つです。
本作品は、代表的なトリプルストアである Virtuoso を自前で構築するための先行事例をさらに改良したものであり、技術面でのアプローチのしやすさと、インストール及びデータロードの速度向上が見込める優れたアイデアとして評価いたしました。
他の様々なクラウドサービスにも応用できそうです。今後さらなる発展を期待いたします。</p></blockquote><p>オンラインではあったが，事務局委員長の武田 英明氏からの言葉とリモート賞状授与を受けて少し涙が出てきた．
去年から色々散々な人生をやっていてメンタルはだいぶ暗いのだが，久々に誰かに褒めてもらって報われた気持ちになり感極まってしまった……</p><blockquote class="twitter-tweet" align="center" data-width="550" data-lang="ja" data-dnt="true"><p lang="ja" dir="ltr"><a href="https://twitter.com/hashtag/lodc2021?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener noreferrer">#lodc2021</a> LODプロモーション賞ありがとうございました！<br><br>来年はデータ作成＆活用の両部門に応募してさらに上を目指せるように今後も精進します！</p>— ありがとう上木敬☁ (@Ningensei848) <a href="https://twitter.com/Ningensei848/status/1502928431943598081?ref_src=twsrc%5Etfw" target="_blank" rel="noopener noreferrer">2022年3月13日</a></blockquote><blockquote class="twitter-tweet" align="center" data-width="550" data-lang="ja" data-dnt="true"><p lang="ja" dir="ltr">tkk先生の受賞式での姿を見てﾃﾞｭﾌﾌつってる<a href="https://t.co/ONPBqaP7YS" target="_blank" rel="noopener noreferrer">https://t.co/ONPBqaP7YS</a></p>— ありがとう上木敬☁ (@Ningensei848) <a href="https://twitter.com/Ningensei848/status/1502880947892412416?ref_src=twsrc%5Etfw" target="_blank" rel="noopener noreferrer">2022年3月13日</a></blockquote><blockquote class="twitter-tweet" align="center" data-width="550" data-lang="ja" data-dnt="true"><p lang="ja" dir="ltr"><a href="https://t.co/nFxvNenXSm" target="_blank" rel="noopener noreferrer">https://t.co/nFxvNenXSm</a><br><br>Google site だけども，高校生が頑張って作ってるのすごいなぁ………時代がどんどん</p>— ありがとう上木敬☁ (@Ningensei848) <a href="https://twitter.com/Ningensei848/status/1502895529948172297?ref_src=twsrc%5Etfw" target="_blank" rel="noopener noreferrer">2022年3月13日</a></blockquote><blockquote class="twitter-tweet" align="center" data-width="550" data-lang="ja" data-dnt="true"><p lang="ja" dir="ltr">在野のヤベェおっさん，金を時間に任せてまじでやべ～～～～＾ことをしててすごい，俺もこれになりたいが？</p>— ありがとう上木敬☁ (@Ningensei848) <a href="https://twitter.com/Ningensei848/status/1502901065796120578?ref_src=twsrc%5Etfw" target="_blank" rel="noopener noreferrer">2022年3月13日</a></blockquote><blockquote class="twitter-tweet" align="center" data-width="550" data-lang="ja" data-dnt="true"><p lang="ja" dir="ltr">ピコピコプラネットはマジですごいので見習いたいね<br><br>☆ピコピコプラネット☆ SPACE - SPARQLクエリ共有サイト<a href="https://t.co/ffUP2ulYR6" target="_blank" rel="noopener noreferrer">https://t.co/ffUP2ulYR6</a></p>— ありがとう上木敬☁ (@Ningensei848) <a href="https://twitter.com/Ningensei848/status/1502913211477024768?ref_src=twsrc%5Etfw" target="_blank" rel="noopener noreferrer">2022年3月13日</a></blockquote><hr><p>そろそろ気持ちを切り替えて色々人生をやっていかねばならない．</p><p>ちょうどシンポジウムの最後で Code for Japan さんの採用情報があった．
ここで一つ応募してみようと思う．</p><p><a href="https://recruit.code4japan.org/" target="_blank" rel="noopener noreferrer">https://recruit.code4japan.org/</a></p><p>人生始めて行かね～～～とな～～</p>]]></content>
        <author>
            <name>Kiai</name>
            <email>k.kubokawa@klis.tsukuba.ac.jp</email>
            <uri>https://twitter.com/Ningensei848</uri>
        </author>
        <category label="LOD" term="LOD"/>
        <category label="オープンデータ" term="オープンデータ"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[netkeiba のデータをスクレイピングして LOD 化する（２）]]></title>
        <id>/2022/03/12/</id>
        <link href="https://ningensei848.github.io/2022/03/12/"/>
        <updated>2022-03-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[python でデータを扱うにあたり，Notebook を使わない選択肢はないだろう．]]></summary>
        <content type="html"><![CDATA[<p>python でデータを扱うにあたり，Notebook を使わない選択肢はないだろう．
Google が提供する <a href="https://colab.research.google.com" target="_blank" rel="noopener noreferrer">Colaboratory</a> を使って，「下書き」的にコードを書いていく．</p><p><a href="https://colab.research.google.com/" target="_blank" rel="noopener noreferrer">https://colab.research.google.com/</a></p><p>まずは，木曜の夕方には確定する出走馬情報を得るアプローチを考える．
netkeiba.com においては，開催レースの一覧が <code>/top/race_list.html</code> で提供されている．
ただし，これをそのまま cURL 等でページ取得しても，各日程ごとの情報は得られない．
なぜなら，jQuery で Ajax を頑張っているからだ（DevTool で調べてみるとわかるだろう）．</p><p>サーバクライアントモデルよろしく，API エンドポイントにリクエストを投げてデータだけを得てクライアント側で出力しているのかと思いきや，HTML コードの断片を貰って埋め込んでいるような方式のように見える．
同じく DevTool でネットワークを監視してみると，<code>/top/race_list_sub.html</code> に対して <code>?kaisai_date=YYYYMMDD</code> でリクエストを投げていることがわかった．</p><p><a href="https://race.netkeiba.com/top/race_list_sub.html?kaisai_date=YYYYMMDD" target="_blank" rel="noopener noreferrer">https://race.netkeiba.com/top/race_list_sub.html?kaisai_date=YYYYMMDD</a></p><p>留意すべきは，開催済み・開催前のレースの両方とも同一の形式で取得できてしまうことである．
すなわち，ベースとなる HTML が <code>/race/shutuba.html</code> と <code>/race/result.html</code> というように異なっている．</p><p>となると，beautifulSoup に喰わせるのではなく正規表現で <code>race_id</code> を引っこ抜いてくるのが賢いだろう．</p><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</h5></div><div class="admonition-content"><p>レース ID については，例えば凱旋門賞や香港スプリントといった中央競馬以外のレースの場合には，例外処理が必要かもしれない．
…………と思ったのもつかの間，ご丁寧に <code>?race_id=YYYYPPNNDDRR&amp;rf=race_list</code> といった具合に <code>&amp;rf=race_list</code> が手がかりとなってくれている．
ありがたくこれを活用し，正規表現でレース ID のみをぶっこ抜く．</p></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="スクレイピングの負荷分散">スクレイピングの負荷分散<a class="hash-link" href="#スクレイピングの負荷分散" title="Direct link to heading">​</a></h2><p>で，実際にスクレイピングしていくにあたって，<strong>負荷分散を考えねばならない</strong>という問題がある．
例えば手元の Python プログラムで秒間 150 回のリクエストをサーバに送ったとしよう．
それによって自分は素早く大量のデータを得ることができるが，そんな大量のリクエストを捌かねばならない（しかも無償で！）サーバ側はたまったものではない．
多くの場合，ロードバランサーなり監視システムなりが同一 IP からの異常リクエストを検知し遮断する措置が取られる．</p><p>データを集めるためには「お行儀よく」スクレイピングのコードを書く必要がある……とこれまでは思っていたが，あまりにも時間がかかりすぎる．
待っている時間にもサーバ利用料金は発生してしまうし，資源の無駄である．
どうにか回避するためには，<strong>複数箇所から同時並行に</strong>リクエストを投げれば良いことに気づくだろう．
各プロセスが 2 秒ずつ待機せねばならなかったとしても，それが 10 プロセス同時であれば効率は十倍になる（進次郎構文）．</p><p>最近になって，これを手軽に実装できるのは，<a href="https://cloud.google.com/api-gateway" target="_blank" rel="noopener noreferrer">Google API Gateway</a> + <a href="https://cloud.google.com/functions" target="_blank" rel="noopener noreferrer">Google Cloud Function</a> の組み合わせであろうことに気がついた．
単にリクエストのプロキシになってもらうというだけなので，特段難しいコードは存在しない（Fetch のみ）．
ちょうど先月にプレビュー版が出た Cloud Function Gen 2 を試しつつ，API Gateway で wrap して，Python からでも並列処理がしやすい方法を検討する．
<del>（ただし，現時点ではエントリポイントがパブリックにオープンになってて悪用の恐れがあるので，認証かなんかをきちんと検証する必要がある）</del></p><p>API Gateway に対するリクエストが膨大になっても問題ないっぽいし，認証に API キー使えば不特定多数にオープンになるリスクも減らせる．</p><blockquote class="twitter-tweet" align="center" data-width="550" data-lang="ja" data-dnt="true"><p lang="ja" dir="ltr">このドキュメントによると，「100秒あたり1000万リクエストまで」がレート制限っぽいので，個人が100並列でスクレイピングかける程度だと全然問題ないっぽい，やったぜ<a href="https://t.co/uAiCr1YTo8" target="_blank" rel="noopener noreferrer">https://t.co/uAiCr1YTo8</a> <a href="https://t.co/gFPWwErK6k" target="_blank" rel="noopener noreferrer">https://t.co/gFPWwErK6k</a></p>— ありがとう上木敬☁ (@Ningensei848) <a href="https://twitter.com/Ningensei848/status/1502576964527267840?ref_src=twsrc%5Etfw" target="_blank" rel="noopener noreferrer">2022年3月12日</a></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="api-gateway-と-cloud-function">API Gateway と Cloud Function<a class="hash-link" href="#api-gateway-と-cloud-function" title="Direct link to heading">​</a></h2><p>色々こねこねして，使いたい名前をファイルに列挙するだけで API Gateway が生えるようにした．</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">$ </span><span class="token function" style="color:#d73a49">npm</span><span class="token plain"> run exec:all --name</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NAME --project</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">projectName</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ </span><span class="token function" style="color:#d73a49">npm</span><span class="token plain"> run gateway:describe --name</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NAME --project</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">projectName</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ </span><span class="token function" style="color:#d73a49">npm</span><span class="token plain"> run gateway:describe:api --name</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NAME --project</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">projectName</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ gcloud services </span><span class="token builtin class-name">enable</span><span class="token plain"> my-api-123abc456def1.apigateway.my-project.cloud.goog</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><code>gateway:describe</code> でエンドポイントが得られ，<code>gateway:describe:api</code> で<strong>マネージドサービスプロパティ</strong>が表示される．
<code>gcloud services enable ${managedServiceUri}</code> で API を有効化することで，エンドポイントに対して API キーによる認証付きでリクエストができるようになる．</p><p>また，その API キーについては，コンソール上の<a href="https://console.cloud.google.com/apis" target="_blank" rel="noopener noreferrer">API とサービス</a>で取得する．
〈呼び出せるキーの制限〉は，<code>openapi.yaml</code> で <code>info.title</code> で指定したもの<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>をドロップダウンから選べば良い．</p><p>これでエンドポイントには，クエリパラメータとして <code>key=${API_KEY}</code> を渡さないと拒否されるようになった．</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="リクエストのプロキシ">リクエストのプロキシ<a class="hash-link" href="#リクエストのプロキシ" title="Direct link to heading">​</a></h2><p>いよいよ python コードを書くことになるが，リクエストを投げるのは対象とする URL ではなく，API Gateway のエンドポイントであり，URL はクエリパラメータとして <code>url=${TARGET_URL}</code> という形で渡す．
返り値は特に加工してないため，単純にリクエストを投げたのと同様の HTML が返る．
一箇所から規定時間内に規定回数以上のリクエストを投げると，DoS 攻撃と勘違いされてアクセス制限の憂き目に合うが，Cloud Functions で負荷分散＋ Gateway でプロキシしてやれば，問題は生じないかも？</p><p>まだ検証していない段階だが，ひとまず API Gateway の自動生成までは出来たので一区切り．</p><div class="footnotes"><hr><ol><li id="fn-1"><a href="https://cloud.google.com/api-gateway/docs/secure-traffic-gcloud#creating_an_api_config" target="_blank" rel="noopener noreferrer">API 構成の作成</a>を参照<a href="#fnref-1" class="footnote-backref">↩</a></li></ol></div>]]></content>
        <author>
            <name>Kiai</name>
            <email>k.kubokawa@klis.tsukuba.ac.jp</email>
            <uri>https://twitter.com/Ningensei848</uri>
        </author>
        <category label="python" term="python"/>
        <category label="スクレイピング" term="スクレイピング"/>
        <category label="競馬" term="競馬"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[netkeiba のデータをスクレイピングして LOD 化する（１）]]></title>
        <id>/2022/03/08/</id>
        <link href="https://ningensei848.github.io/2022/03/08/"/>
        <updated>2022-03-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[改めて，Netkeiba からスクレイピングをやっていく．]]></summary>
        <content type="html"><![CDATA[<p>改めて，Netkeiba からスクレイピングをやっていく．
Python でやるのは，リクエストに間隔を開ける都合上，多少時間がかかっても問題がないことや，DataFrame 系の資産を使い回せることが利点として挙げられる</p><p><a href="https://github.com/Ningensei848/ml4keiba" target="_blank" rel="noopener noreferrer">https://github.com/Ningensei848/ml4keiba</a></p><p><a href="https://github.com/Ningensei848/ml4keiba" target="_blank" rel="noopener noreferrer"><img src="https://4.bp.blogspot.com/-7KSDS7fjQZU/U1T4Hfdp7aI/AAAAAAAAfds/kxPMlCXrIkk/s200/seiza13_hebitsukai.png" alt="ギリシャ神話に出てくる蛇を持つ医者アスクレーピオス（へびつかい座）"></a></p><p>前回までの反省として，<strong>何も考えずひたすらにスクレイピングしていた</strong> というものが挙げられる．すなわち，不要なデータまでも「必要かもしれない」と集めて時間を浪費していた．これは，リストを作ってそれを一つずつ実行する設計になっていたことが原因だ．</p><p>例えば本日はちょうど「弥生賞ディープインパクト記念」をやっている．
このレースの予想をデータ分析によって行う場合に，「天皇賞（春）」のデータが必要だろうか？</p><p>最初から「完璧」を目指して作ろうとするからポンコツ不完全にしかならないという現実があるので，実際のレース時期に合わせてちいさくはじめていくべきだろう．</p><p>今回で言えば，弥生賞に出てくる馬それぞれの過去のレースを探ったり，過去の弥生賞についてデータを浚ったりするのが常道といえる．</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="てがかり">てがかり<a class="hash-link" href="#てがかり" title="Direct link to heading">​</a></h2><p>まず，レースに出走する馬の一覧を入手する必要がある．</p><p>netkeiba では，<code>YYYYPPNNDDRR</code> という ID でレースごとの情報が管理されている．</p><ul><li><code>YYYY</code>: 開催年度</li><li><code>PP</code>: 会場コード</li><li><code>NN</code>: N 回目</li><li><code>DD</code>: 第 D 日</li><li><code>RR</code>: 第 R レース</li></ul><p>といった具合である．</p><p>過去の調査では，<code>PP</code> が厄介なことに，「地方」「海外」も雑多に含まれることがあり，単にインクリメントしているわけではないらしい．</p><p>（と，これを調べる過程で netkeiba.com が設立されたのが 1999 年末だということを知った，<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>そろそろ四半世紀にもなる上に前世紀からデータ提供をやってるんだからすげぇ）</p><p><a href="https://ascii.jp/elem/000/000/306/306735/" target="_blank" rel="noopener noreferrer">ASCII.jp：ネットドリーマーズ、競馬のポータルサイト“netkeiba.com”を開設</a></p><p>これを書いているのは火曜日だが，週末に行われるレースについては現時点で出走馬も枠順も決まっていなかった．ただし週末に行われる重賞レースに登録されている馬の一覧は見ることができた．調べてみると netkeiba においては，以下のようなスケジュールで情報が書き換わるようだ：</p><ul><li>前週の日曜（G1 は前々週）　特別レース登録馬を公開</li><li>（netkeiba 独自）水曜 20 時ごろ　取材などからわかった水曜時点で出走意思のある馬（想定馬一覧）</li><li>木曜 16 時ごろ　　　　出走馬確定</li><li>レース前日 10 時ごろ　枠順確定</li></ul><p><a href="https://www.jra.go.jp/kouza/yougo/w333.html" target="_blank" rel="noopener noreferrer">特別レース（特別競走）</a> とは，"一般競走と違って、特別登録を必要とする競走。特別競走には、現在の中央競馬ではすべてレース名がつけられている。また重賞競走も特別競走のなかに含まれる" ものであるらしい．また<a href="https://www.jra.go.jp/kouza/yougo/w320.html" target="_blank" rel="noopener noreferrer">一般競走</a>とは，特別競走以外，すなわち新馬戦やオープン戦，条件戦などのことを指す<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>．</p><p>つまり，おおよそほとんどの場合において，木曜日の夜あたりから情報収集を始めるのが良いということがわかる．
また，枠順が決まらないことには予想も固まらないことを考えると，レース前日の夕方から準備し始めても十分に間に合う．
金曜＋土日でレース本番への対策を行ない，それ以外の日には別の情報収集＋メンテナンスや振り返りというペース配分になるだろう．</p><p>（※これは中央競馬だけにフォーカスした場合の話で，平日もガンガン走っている地方競馬はまた別の話）</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="出走馬一覧をシードとして">出走馬一覧をシードとして…<a class="hash-link" href="#出走馬一覧をシードとして" title="Direct link to heading">​</a></h2><p>netkeiba では，出走馬ごとに ID が振られており，それによって血統や戦績，厩舎や騎手などの情報を管理している．</p><p>日本生まれ，かつ出生情報がきちんと揃っている場合には <code>https://db.netkeiba.com/horse/YYYYXXXXXX</code> と表記される（<code>YYYY</code> は生年）．
一方で海外産馬などの都合で情報が不明瞭な場合は <code>https://db.netkeiba.com/horse/000a00033a</code> といった ID が振られている（ちなみにこの URL は<a href="https://db.netkeiba.com/horse/000a00033a" target="_blank" rel="noopener noreferrer">サンデーサイレンス</a>）．
<code>000a</code> はほぼ共通だが，それ以外についてはひと目見ただけではあまり共通項が見えてこない．</p><hr><p>各馬について，スクレイピングの対象となるのは以下の５つのページである：</p><ul><li><code>/</code>: プロフィール</li><li><code>/result</code>: 競走成績</li><li><code>/ped</code>: 血統</li><li><code>/sire</code> or <code>/mare</code>: 産駒の競走成績（繁殖入りした馬のみ）</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="プロフィール">プロフィール<a class="hash-link" href="#プロフィール" title="Direct link to heading">​</a></h3><p>各馬のトップページにアクセスするとまっ先に目に入るのがプロフィールである．
生まれに関する基礎情報や写真なんかもおいてあるし，他のページに詳しく掲載される情報も概要がまとめてある．</p><p>ここで収集すべきは，<code>div.horse_title</code>, <code>div.db_photo_box&gt;img</code>, <code>dl.tekisei&gt;dd&gt;table.tekisei_table</code>, <code>div.db_prof_table</code> の４箇所だ．
それぞれ，「名前」「写真」「適正評価」「プロフィール」が掲載されている．</p><p>また，「繁殖入りできたかどうか？」の判定のために〈産駒成績〉タブの有無も見つけられるようにしたい．
<code>ul.db_detail_menu</code> を探るといいだろう．</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="競走成績">競走成績<a class="hash-link" href="#競走成績" title="Direct link to heading">​</a></h3><p>競走成績一覧のページでは，その馬が一つでもレースに出ていれば，結果が表として出力される．
<code>table.db_h_race_results</code> を収集すればいいだろう．
未出走の場合や海外産馬の場合には表がないこともある．例外処理には気をつけたい．</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="血統">血統<a class="hash-link" href="#血統" title="Direct link to heading">​</a></h3><p>こちらも同じく真ん中にデカデカと血統表が出力される．
<code>table.blood_table</code> を収集すればいいだろう．
名前等はこの時点では収集せず……と思ったが，後々必要になる気もしてきた．</p><p>また，兄弟等の近親についても情報があるが，敢えてこれを探りに行くのは骨が折れる．
ある程度データを集めてから，自前で謹慎を探せるようにするほうが良いと思われる．</p><p>表の組み方が特殊なことに留意．</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="産駒成績">産駒成績<a class="hash-link" href="#産駒成績" title="Direct link to heading">​</a></h3><p>牡馬であれば <code>/sire</code>, 牝馬であれば <code>/mare</code> のページを持つ場合がある．
これは，繁殖入りできるほど血統的に期待される，或いは競走成績が良かった馬ということで，勝ち馬予想のためには必要不可欠な要素ではある．
が，あくまで統計的な情報でしかなく，最初から予想アルゴリズムに組み込むのは難しいかもしれない．</p><p>産駒成績のページの存在判定は各馬のトップページでもタブを見ればできるはずなので，一旦保留して関知しないこととする．</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="一旦まとめ">一旦まとめ<a class="hash-link" href="#一旦まとめ" title="Direct link to heading">​</a></h2><p>netkeiba からスクレイピングしてくるための検討をした．</p><p>スクレイピング処理の部分と LOD 化する部分の話がまだ書けていないが，一旦 Puslish しておく．
（まぁどうなるかわからないが，前編ということとする）</p><p>Python によるスクレイピング処理とはすなわち， (1) <code>requests</code> （あるいは <code>aiohttp</code> ）でページコンテンツを取得し，(2) それを BeautifulSoup <sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup> にして (3) <code>pandas</code> でテーブルとか引っこ抜くことである．
この辺は以前に何度もやっているのでそのへんの資産を使い回せたら嬉しいな……</p><p>LOD 化するというのは，どうしようかまだ悩むところである．
オープンデータにすることを考えると，いちいち事前知識が必要になる TTL 形式のみで提供するのは避けたい．
となると JSON-LD か？となるがこれもこれでファイル容量の無駄遣いが大きい気がする……</p><p>TSV で提供 &amp; TTL への変換ツールも提供という形がもっともスマートな解だと信じたい．</p><div class="footnotes"><hr><ol><li id="fn-1">というかそんな時期の記事にまだアクセスできる ASCII の根性がスゲェ，尊敬に値する<a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"><a href="https://www.jra.go.jp/kouza/yougo/c10020.html" target="_blank" rel="noopener noreferrer">レースの種類、条件など（競馬用語辞典）JRA</a> を参照のこと<a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3">現時点 (2022/03/08) の<a href="https://pypi.org/project/beautifulsoup4/4.10.0/" target="_blank" rel="noopener noreferrer">最新版は Python 3.8 前提の 4.10.0</a> である（なお日本語訳は 4.2.0 までしかない模様）<a href="#fnref-3" class="footnote-backref">↩</a></li></ol></div>]]></content>
        <author>
            <name>Kiai</name>
            <email>k.kubokawa@klis.tsukuba.ac.jp</email>
            <uri>https://twitter.com/Ningensei848</uri>
        </author>
        <category label="python" term="python"/>
        <category label="スクレイピング" term="スクレイピング"/>
        <category label="競馬" term="競馬"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[個人ブログを Docusaurus で再始動する]]></title>
        <id>/2022/03/03/</id>
        <link href="https://ningensei848.github.io/2022/03/03/"/>
        <updated>2022-03-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[もともとは Next.js の知見を貯めるために vercel では普通のアプリとして，GitHub Pages では SSG としてブログをつくる予定だった．が， Docusaurus で全然いいしむしろデザイン対応の手間を考えたら Next.js で全部自分でやるのは（無限にこだわり続けてしまって）完成しない！という結論を得た．]]></summary>
        <content type="html"><![CDATA[<p>もともとは Next.js の知見を貯めるために vercel では普通のアプリとして，GitHub Pages では SSG としてブログをつくる予定だった．が， Docusaurus で全然いいしむしろデザイン対応の手間を考えたら Next.js で全部自分でやるのは（無限にこだわり続けてしまって）完成しない！という結論を得た．</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="そもそも">そもそも<a class="hash-link" href="#そもそも" title="Direct link to heading">​</a></h2><p>Docusaurus もこれまで $\alpha$ 版には触れてきたが，Next.js における <code>getStaticProps</code> がないからローカルのファイルデータのやり取りができなくて辛い，と思い込んでしまっていた．しかし，実際にはプラグインを時前実装することで実現できることがわかった（それも正直どうなのって感じだが）</p><p>Docusaurus と Next.js のハイブリッドも検討したが，あまり親和性がないと言うか，そこまでしてやるメリットがないとは感じていた．</p><p>改めて，Next.js を使うのは ISR をやれるからという利点があるときだけで，そうでないときは Docusaurus を使ったほうが後々の拡張性が高いように感じた（特にレイアウトやらダークモードなどデザイン苦手マンには）</p><p>で，この GitHub Pages は，その性質上今後も SSG のみ対応すると思われる．じゃあ Next.js に縋り付く意味なくない？よって Docusaurus への移行を決意した．</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="今後">今後<a class="hash-link" href="#今後" title="Direct link to heading">​</a></h2><p>なにかしらフィーチャーすべきものについては，「ドキュメント」をつくってまとめる．幸い「Multi Sidebar」なるものはすでにある（上部バーのドロップダウンがすぐに作れてアド）</p><p>ブログは，ルーティングをもう少し留意する必要はあると思いつつも，まぁこれでいいかという妥協点を保っている．そんなに何度も何度も更新するわけではないし……</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="最後に">最後に<a class="hash-link" href="#最後に" title="Direct link to heading">​</a></h2><p>はやいとこいくつか記事作って Google Adsense 通るようにしたいね</p><hr><p>↓ 　これは URL 貼り付けるだけでツイートが埋め込み表示されて便利ね～というデモンストレーションです</p><blockquote class="twitter-tweet" align="center" data-width="550" data-lang="ja" data-dnt="true"><p lang="ja" dir="ltr">余裕をもってバスに乗る🚌</p>— ありがとう上木敬☁ (@Ningensei848) <a href="https://twitter.com/Ningensei848/status/1477068019612659712?ref_src=twsrc%5Etfw" target="_blank" rel="noopener noreferrer">2022年1月1日</a></blockquote><p>↓ 　あとこの辺に SNS 向けの共有ボタンがいくつか設置されてるはず</p>]]></content>
        <author>
            <name>Kiai</name>
            <email>k.kubokawa@klis.tsukuba.ac.jp</email>
            <uri>https://twitter.com/Ningensei848</uri>
        </author>
        <category label="日記" term="日記"/>
    </entry>
</feed>